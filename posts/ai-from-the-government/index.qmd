---
title: "AI from the government?"
date: 2025-11-18
categories: ['ideas', 'ai']
tags: ['public ai', 'governance', 'ethics']
description: | 
  An introduction and overview of the concept of Public AI and its implications for society.
bibliography: references.bib
---

We can probably agree that artificial intelligence[^1] is not a guaranteed hole in one and there are some valid concerns for how it may negatively effect us humans. Therefore we aught come up with ways in which we can do it right and one of these ideas is called Public AI. This idea comes from a collection of people from academia and industry and is 'centrally' organized by [@publicaiPublicAIWhite2024]. The core idea is that AI systems should be viewed like public goods, similar to water, electricity and roads. This is essential as AI systems are becoming more integrated and thus critical to our daily lives [^2], where making them a public good adds a quality floor and price ceiling to enable equitable access and prevent exclusion. Changing the paradigm of AI development from private profit maximization to public good maximization is a switch of incentives that can push AI development in a more positive direction.

[^1]: I have a love hate relationship these days with the word AI, particularly how it is used in the context of governance and policy discourse. It always amuses me (visualize an accountant freaking out and just saying "money" instead of taxes, income etc) when people discuss broadly concerns of various computer systems under the lump title of 'AI', I assume that it means they don't really know any of words for. Therefore I will demonstrate that I know atleast two more buzz words by suggesting that I could of replaced AI with agentic LLM for this sentence.

[^2]: Generally speaking it seems the more things are used in a society the more people without them are left out / left behind. Ergo as more people use AI the ones who can't access it due to money, logistics etc are left behind

## Its all about incentives

Things are optimizers. Particles are just trying to get to lower energy states. Humans are trying to minimize discomfort and maximize pleasure. Societies optimizing for? What are companies optimizing for? Atleast for companies it is quite safe to say they are optimizing for profit, specifically near term profit (\< 50 years). These goals are good proxies for some handy attributes like efficiency, survival and so one, yet they aren't necessarily aligned with Utilitarian [@millUtilitarianism1879] goals or really any of the goals of a standard philosophical ethical system [@aristotleAristotlesNicomacheanEthics350B.C.E; @kantImmanuelKantGroundwork1785] [^3]. Generally speaking profit maximization leads to short term thinking, exploitation of all resources (human, natural etc) and ignoring externalities. These are not good foundations for a driver of positive change in a civilization let alone a powerful technology like AI.

[^3]: Forgive me dearly for I am no Philosopher and just want to make the point that profit is not a good goal.

The current AI development framework is mainly of developing an AI system that is designed to maximize profit for shareholders. This leads to many instrumental goals at both the organizational level and the AI level. At the AI level we can imagine it learns goals like maximizing user engagement, manipulating users preferences and generally all of the negatives things associated with social media. Social media has pretty terrible outcomes across society and has AI has more negative potential then that[^4]. At the organizational level we can use a framework from the Collective Intelligence Project [@collectiveintelligenceprojectCIP+Whitepaper] that states organizations will result in negative outcomes for society by sacrificing safety for progress, participation for safety, or progress participation.

[^4]: Think how much damage recommender systems have done (causes genocides [@amnestyinternationalMyanmarSocialAtrocity2022], polarization [@brownEchoChambersRabbit2022]...), a system with a simple goal of find the next piece of content that the user would like the most

## A better incentive

Ok so if profit, power and growth are not good moral goals and not good societal goals what should our AI goals be? Well on one extreme is optimizing for maximum sum of happiness minus suffering (AKA utilitarianism)[^5], yet to avoid pitfalls and problems when using a philosophical framework, I will instead use the concept of public good. The definition of public good is something that everyone can use without it effecting someones else's ability to use it. It's the clean air we breath, which is the same air that companies pollute to drive profit (which an example of ignoring externalities). Therefore we are left with the goal of maximizing access minus exclusion.

[^5]: Well if i understand it technically a weighted sum where the weights are how much the evaluator cares about the thing. Like for instance if I was to evaluate it I would put no weight on the happiness of limestone rocks as I care about them much.

One can view government and governing bodies in general as entities that provides incentive tweaks to society. That if done correctly let societies collective efforts move towards the future we want. Thus if we move AI development in-house, or atleast the responsibility of the collective (like roads, power grid etc) we will realign the incentives of AI development from profit maximization to public good maximization.

Importantly the goal of public good maximization generally has better instrumental goals associated with it. Namely being help the public. The previous wave of technological development was driven by profit and private enterprises which has resulted in much better advertising and entertainment, yet also increasing inequality and polarization. All while ignoring much needed problems like climate change, public health and the global poor. When operating within the framework of building a public good you are *for the public good*, you are not judged on how much money you make but rather how much good you do for the public[^6]. For example The AI bot that talks to users for hours on end and causes them to quite their job and spend all their money on it, great for business yet terrible for society so not incentivized.

[^6]: Lets ignore problems of crony capitalism for now

## How to do it

Fixing it at the incentive level is really attractive. We just write this document of how things should be done and then everyone follows it, and i get my flying car by christmas! Maybe its kinda like that, but more likely those pesky road bumps on the road to the future are non trivial and might be more like building a new road from scratch than just driving over a road bump. Therefore there are some significant technical and social challenges to overcome.

Broadly speaking there are a few key areas that need work so that the government can recapture AI and move it towards public good maximization:

-   **Public AI infrastructure:** Due to large compute and data requirements building AI systems is expensive. Therefore we need public infrastructure that can support the development and deployment of AI systems. Specifically we need public compute and public data. The lack of this is a crucial reason that early startup take that tempting venture capitalist money then have to spend the time trying to make money for them.\
-   **Representative AI development:** *I will expand below as I think this is the most interesting*. But how do we build AI systems both at the application and model level that represent and maintain public and individual values.\
-   **Audits and transparency:** This kind of goes with the above but we need clear ways forward in how we can audit AI systems to ensure they are aligned with whatever we want (but in our case we could say public good maximization). We can somewhat tell people what data we used, algorithm and so on yet that provides information at the wrong abstraction level. It doesn't tell people what they really care about which is "Will it be kind", "Will this bot thingy treat me fairly", "why did it deny my application" etc. Some of this is covered or partially solved by the EU AI act [@ArtificialIntelligenceAct2024], yet more is left out on how to technically do this and practically implement it.\

### Good AI development processes

There are many problems with current AI development these can be broken down into the lifecycle of a classical Machine Learning system.

First is data. Any model is currently trained using some input of information and the information used can have a huge impact on the resultant model. Therefore we need better ways of procuring data that is fair for the everyone. Furthermore and crucially for post training data how do we decide on what sort of model we would want (e.g nationalism vs globalism or self interest vs group interest)? This is where the concept of treating it as part of our collective intelligence can be effective framing and the CIP [@collectiveintelligenceprojectCIP+Whitepaper] works on it alot. One of the key outputs of CIP is how to extract and synthesis values from a large dis homogenous group of people.

Second is training. From the previous step of data it is not always obvious how to use this data to train the model [^7]. This immediately brings us to very technical ideas like RLHF [@christianoDeepReinforcementLearning2023] and DPO [@rafailovDirectPreferenceOptimization2024], however more broadly speaking the question is still open on how to instill the values and expected behaviors into models. There are interesting ideas like Constitutional AI [@baiConstitutionalAIHarmlessness2022] yet not only are they only in early stages they are largely ignored due to lack of direct profit incentive.

Lastly is deployment, which I use in the sense of embedding these AI models into applications that end users interact with as well as the actual hardware it runs on. Here the question is how do we do that in the correct method. For example deploying a chatbot we could follow some framework like [@CITELayeredDefense2025] or other "standard engineering best practices" like processes.

[^7]: Now obviously the first part is just training using next token prediction, yet it is what comes after that is the most interesting bit

## Now remember

Building things is cool. I like building things. I particularly enjoy building computers and software that does really neat things. Yet we are dealing with powerful technologies ^[a cool word for this is transformative technologies] and we may be dealing with something more powerful than ever before. If the foundations of how we are building these systems are not aligned with what we want, then we are simply fooling ourselves into thinking we are going to get something good. We need to align the incentives of AI development with what we humans want at all levels of the process. Not just some small technical post training part. Public AI is one such framework that can help us do that.