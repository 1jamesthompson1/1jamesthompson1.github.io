@misc{AISafetyParadox,
  title = {The {{AI Safety Paradox}}: {{When}} '{{Safe}}' {{AI Makes Systems More Dangerous}}},
  shorttitle = {The {{AI Safety Paradox}}},
  journal = {The Collective Intelligence Project},
  urldate = {2025-11-18},
  abstract = {AI safety is becoming an established field and science. Yet by focusing only on the safety of individual models, AI labs may actually be making the conditions in which they're deployed less safe.},
  howpublished = {https://www.cip.org/blog/safetyparadox},
  langid = {canadian},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/JTCM6AH2/safetyparadox.html}
}

@misc{AIsLeversAre,
  title = {{{AIs}} without Levers Are Inert - by {{James Padolsey}}},
  urldate = {2025-11-21},
  howpublished = {https://blog.j11y.io/2024-07-11\_AIs\_inert/},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/LTBNYY2E/2024-07-11_AIs_inert.html}
}

@techreport{amnestyinternationalMyanmarSocialAtrocity2022,
  title = {Myanmar: {{The}} Social Atrocity: {{Meta}} and the Right to Remedy for the {{Rohingya}}},
  shorttitle = {Myanmar},
  author = {{Amnesty International}},
  year = 2022,
  month = sep,
  number = {ASA 16/5933/2022},
  urldate = {2025-11-21},
  abstract = {Beginning in August 2017, the Myanmar security forces undertook a brutal campaign of ethnic cleansing against Rohingya Muslims. This report is based on an in-depth investigation into Meta (formerly Facebook)'s role in the serious human rights violations perpetrated against the Rohingya. Meta's algorithms proactively amplified and promoted content which incited violence, hatred, and discrimination against [\dots ]},
  langid = {english},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/P5G2R3N4/Amesty International - 2022 - Myanmar The social atrocity Meta and the right to remedy for the Rohingya.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/N9R22YQK/en.html}
}

@book{aristotleAristotlesNicomacheanEthics350B.C.E,
  title = {Aristotles {{Nicomachean Ethics}}},
  author = {{Aristotle}},
  year = {350 B.C.E},
  urldate = {2025-11-18},
  abstract = {Aristotles Nicomachean Ethics},
  langid = {english},
  keywords = {Aristotle Nicomachean Ethics}
}

@misc{ArtificialIntelligenceAct2024,
  title = {Artificial {{Intelligence Act}} ({{Regulation}} ({{EU}}) 2024/1689)},
  year = 2024,
  month = jun,
  urldate = {2025-11-21},
  langid = {english},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/I3T34F62/2024 - Regulation (EU) 20241689 of the European Parliament and of the Council of 13 June 2024 laying down.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/GDBFUIQE/oj.html}
}

@misc{baiConstitutionalAIHarmlessness2022,
  title = {Constitutional {{AI}}: {{Harmlessness}} from {{AI Feedback}}},
  shorttitle = {Constitutional {{AI}}},
  author = {Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Chen, Carol and Olsson, Catherine and Olah, Christopher and Hernandez, Danny and Drain, Dawn and Ganguli, Deep and Li, Dustin and {Tran-Johnson}, Eli and Perez, Ethan and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lukosuite, Kamile and Lovitt, Liane and Sellitto, Michael and Elhage, Nelson and Schiefer, Nicholas and Mercado, Noemi and DasSarma, Nova and Lasenby, Robert and Larson, Robin and Ringer, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and Lanham, Tamera and {Telleen-Lawton}, Timothy and Conerly, Tom and Henighan, Tom and Hume, Tristan and Bowman, Samuel R. and {Hatfield-Dodds}, Zac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and McCandlish, Sam and Brown, Tom and Kaplan, Jared},
  year = 2022,
  month = dec,
  number = {arXiv:2212.08073},
  eprint = {2212.08073},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.08073},
  urldate = {2025-11-17},
  abstract = {As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/ACQD6HIU/Bai et al. - 2022 - Constitutional AI Harmlessness from AI Feedback.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/X2GY35PC/2212.html}
}

@misc{brownEchoChambersRabbit2022,
  type = {{{SSRN Scholarly Paper}}},
  title = {Echo {{Chambers}}, {{Rabbit Holes}}, and {{Algorithmic Bias}}: {{How YouTube Recommends Content}} to {{Real Users}}},
  shorttitle = {Echo {{Chambers}}, {{Rabbit Holes}}, and {{Algorithmic Bias}}},
  author = {Brown, Megan A. and Bisbee, James and Lai, Angela and Bonneau, Richard and Nagler, Jonathan and Tucker, Joshua A.},
  year = 2022,
  month = may,
  number = {4114905},
  eprint = {4114905},
  publisher = {Social Science Research Network},
  address = {Rochester, NY},
  doi = {10.2139/ssrn.4114905},
  urldate = {2025-11-21},
  abstract = {To what extent does the YouTube recommendation algorithm push users into echo chambers, ideologically biased content, or rabbit holes? Using a novel method to estimate the ideology of YouTube videos and an original experimental design to isolate the effect of the algorithm from user choice, we demonstrate that the YouTube recommendation algorithm does, in fact, push real users into mild ideological echo chambers where, by the end of the data collection task, liberals and conservatives received different distributions of recommendations from each other, though this difference is small. While we find evidence that this difference increases the longer the user followed the recommendation algorithm, we do not find evidence that many go down `rabbit holes' that lead them to ideologically extreme content. Finally, we find that YouTube pushes all users, regardless of ideology, towards moderately conservative and an increasingly narrow range of ideological content the longer they follow YouTube's recommendations.},
  archiveprefix = {Social Science Research Network},
  langid = {english},
  keywords = {Echo Chambers,Political Polarization,Recommendation Algorithm,Theory Testing,YouTube},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/LHQWU65R/Brown et al. - 2022 - Echo Chambers, Rabbit Holes, and Algorithmic Bias How YouTube Recommends Content to Real Users.pdf}
}

@misc{christianoDeepReinforcementLearning2023,
  title = {Deep Reinforcement Learning from Human Preferences},
  author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
  year = 2023,
  month = feb,
  number = {arXiv:1706.03741},
  eprint = {1706.03741},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.03741},
  urldate = {2025-10-28},
  abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/V554LB5Q/Christiano et al. - 2023 - Deep reinforcement learning from human preferences.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/3EVUHNM5/1706.html}
}

@misc{CITELayeredDefense2025,
  title = {{{CITE}}: {{Layered Defense}} in {{AI Chat Safety}} - by {{James Padolsey}}},
  year = 2025,
  month = nov,
  urldate = {2025-11-18},
  howpublished = {https://blog.j11y.io/2025-11-13\_CITE-AI-Safety/},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/7WPJJET6/2025-11-13_CITE-AI-Safety.html}
}

@misc{collectiveintelligenceprojectCIP+Whitepaper,
  title = {{{CIP Whitepaper}}},
  author = {{Collective Intelligence Project}},
  year = 2024,
  urldate = {2025-11-17},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/MYY4SQMA/CIP+Whitepaper.pdf}
}

@misc{compdemocracyPolis2025,
  title = {Polis},
  author = {{compdemocracy}},
  year = 2025,
  month = nov,
  urldate = {2025-11-16},
  abstract = {:milky\_way: Open Source AI for large scale open ended feedback},
  copyright = {AGPL-3.0},
  howpublished = {The Computational Democracy Project},
  keywords = {civic-tech,data-science,deliberative-democracy,participatory-democracy}
}

@book{kantImmanuelKantGroundwork1785,
  title = {Immanuel {{Kant Groundwork For The Metaphysics Of Morals}}},
  author = {{Kant}},
  year = 1785,
  urldate = {2025-11-18},
  abstract = {TextImmanuel Kant: Groundwork for the Metaphysics of Morals (1785) 1Preface3First Section: Transition from common rational moralcognition to philosophical moral cognition9Second Section: Transition from popular moral philosophyto the metaphysics of morals22Third Section: Transition from the metaphysics of moralsto the critique of pure practical reason63Essays1. Why Study Kant's Ethics?83J. B. Schneewind2. Acting from Duty92Marcia Baron3. Kantianism for Consequentialists111Shelly Kagan4. What Is Kantian Ethics?157Allen W. Wood},
  copyright = {https://creativecommons.org/publicdomain/mark/1.0/},
  langid = {english},
  keywords = {Philosophy}
}

@misc{meinkeFrontierModelsAre2025c,
  title = {Frontier {{Models}} Are {{Capable}} of {{In-context Scheming}}},
  author = {Meinke, Alexander and Schoen, Bronson and Scheurer, J{\'e}r{\'e}my and Balesni, Mikita and Shah, Rusheb and Hobbhahn, Marius},
  year = 2025,
  month = jan,
  number = {arXiv:2412.04984},
  eprint = {2412.04984},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.04984},
  urldate = {2025-11-16},
  abstract = {Frontier models are increasingly trained and deployed as autonomous agent. One safety concern is that AI agents might covertly pursue misaligned goals, hiding their true capabilities and objectives - also known as scheming. We study whether models have the capability to scheme in pursuit of a goal that we provide in-context and instruct the model to strongly follow. We evaluate frontier models on a suite of six agentic evaluations where models are instructed to pursue goals and are placed in environments that incentivize scheming. Our results show that o1, Claude 3.5 Sonnet, Claude 3 Opus, Gemini 1.5 Pro, and Llama 3.1 405B all demonstrate in-context scheming capabilities. They recognize scheming as a viable strategy and readily engage in such behavior. For example, models strategically introduce subtle mistakes into their responses, attempt to disable their oversight mechanisms, and even exfiltrate what they believe to be their model weights to external servers. Additionally, this deceptive behavior proves persistent. When o1 has engaged in scheming, it maintains its deception in over 85\% of follow-up questions and often remains deceptive in multi-turn interrogations. Analysis of the models' chains-of-thought reveals that models explicitly reason about these deceptive strategies, providing evidence that the scheming behavior is not accidental. Surprisingly, we also find rare instances where models engage in scheming when only given a goal, without being strongly nudged to pursue it. We observe cases where Claude 3.5 Sonnet strategically underperforms in evaluations in pursuit of being helpful, a goal that was acquired during training rather than in-context. Our findings demonstrate that frontier models now possess capabilities for basic in-context scheming, making the potential of AI agents to engage in scheming behavior a concrete rather than theoretical concern.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/4HWXK46Z/Meinke et al. - 2025 - Frontier Models are Capable of In-context Scheming.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/E8PCV7Y2/2412.html}
}

@book{millUtilitarianism1879,
  title = {Utilitarianism},
  author = {Mill, John Stuart},
  year = 1879,
  urldate = {2025-11-18},
  copyright = {Public domain in the USA.},
  langid = {english},
  lccn = {EBook-No. 11224},
  keywords = {Utilitarianism}
}

@misc{publicaiPublicAIWhite2024,
  title = {Public {{AI White Paper}}},
  author = {{Public AI}},
  year = 2024,
  month = aug,
  doi = {10.5281/zenodo.13914560},
  urldate = {2025-11-17},
  abstract = {In this paper, we set out a vision for a different path for AI.  It starts with a recognition that societies don't have to just consume the AI technologies shaping their lives---they can create them.  That's why we call for a new collective enterprise: building AI infrastructure for the common good. Public investments can unleash a wave of innovation, expanding access to better tools, and in time expanding our collective imagination.  The result is a new political economy.  This is Public AI.},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/5N52Z9MK/Public AI - Public AI White Paper.pdf.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/H2CLMI63/edit.html}
}

@misc{rafailovDirectPreferenceOptimization2024,
  title = {Direct {{Preference Optimization}}: {{Your Language Model}} Is {{Secretly}} a {{Reward Model}}},
  shorttitle = {Direct {{Preference Optimization}}},
  author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea},
  year = 2024,
  month = jul,
  number = {arXiv:2305.18290},
  eprint = {2305.18290},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.18290},
  urldate = {2025-10-25},
  abstract = {While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/IVLQ6B7N/Rafailov et al. - 2024 - Direct Preference Optimization Your Language Model is Secretly a Reward Model.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/NMSA44TU/2305.html}
}

@misc{schoenStressTestingDeliberative2025,
  title = {Stress {{Testing Deliberative Alignment}} for {{Anti-Scheming Training}}},
  author = {Schoen, Bronson and Nitishinskaya, Evgenia and Balesni, Mikita and H{\o}jmark, Axel and Hofst{\"a}tter, Felix and Scheurer, J{\'e}r{\'e}my and Meinke, Alexander and Wolfe, Jason and van der Weij, Teun and Lloyd, Alex and {Goldowsky-Dill}, Nicholas and Fan, Angela and Matveiakin, Andrei and Shah, Rusheb and Williams, Marcus and Glaese, Amelia and Barak, Boaz and Zaremba, Wojciech and Hobbhahn, Marius},
  year = 2025,
  month = sep,
  number = {arXiv:2509.15541},
  eprint = {2509.15541},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2509.15541},
  urldate = {2025-10-28},
  abstract = {Highly capable AI systems could secretly pursue misaligned goals -- what we call "scheming". Because a scheming AI would deliberately try to hide its misaligned goals and actions, measuring and mitigating scheming requires different strategies than are typically used in ML. We propose that assessing anti-scheming interventions requires at least (1) testing propensity to scheme on far out-of-distribution (OOD) tasks, (2) evaluating whether lack of scheming is driven by situational awareness, and (3) checking for robustness to pre-existing misaligned goals. We use a broad category of "covert actions" -- such as secretly breaking rules or intentionally underperforming in tests -- as a proxy for scheming, and design evaluations for covert actions. We then stress-test deliberative alignment as a case study for anti-scheming. Across 26 OOD evaluations (180+ environments), deliberative alignment reduces covert action rates (OpenAI o3: 13\%-{$>$}0.4\%) but does not fully eliminate them. Our mitigation is also able to largely stop agents from pursuing a hidden goal previously trained into the model, but we still find misbehavior after additional red-teaming. We find that models' chain-of-thought (CoT) often demonstrates awareness of being evaluated for alignment, and show causal evidence that this awareness decreases covert behavior, while unawareness increases it. Therefore, we cannot exclude that the observed reductions in covert action rates are at least partially driven by situational awareness. While we rely on human-legible CoT for training, studying situational awareness, and demonstrating clear evidence of misalignment, our ability to rely on this degrades as models continue to depart from reasoning in standard English. We encourage research into alignment mitigations for scheming and their assessment, especially for the adversarial case of deceptive alignment, which this paper does not address.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/P9Q5Q9YV/Schoen et al. - 2025 - Stress Testing Deliberative Alignment for Anti-Scheming Training.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/XBVD76B2/2509.html}
}

@misc{zieglerFineTuningLanguageModels2020,
  title = {Fine-{{Tuning Language Models}} from {{Human Preferences}}},
  author = {Ziegler, Daniel M. and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B. and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  year = 2020,
  month = jan,
  number = {arXiv:1909.08593},
  eprint = {1909.08593},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1909.08593},
  urldate = {2025-11-16},
  abstract = {Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/9T2PCTZ5/Ziegler et al. - 2020 - Fine-Tuning Language Models from Human Preferences.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/C58EKGZD/1909.html}
}
